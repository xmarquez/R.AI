% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/openai-batch.R
\name{openai_format_prompts_for_batch}
\alias{openai_format_prompts_for_batch}
\title{Format Prompts for OpenAI Batch API}
\usage{
openai_format_prompts_for_batch(prompts, model, max_tokens = 300)
}
\arguments{
\item{prompts}{A list of prompts with each prompt containing a list of
messages (role and content).}

\item{model}{The model to use for processing each prompt (e.g.,
"gpt-3.5-turbo-0125").}

\item{max_tokens}{Max number of tokens for each response. Defaults to 300.}
}
\value{
A character string formatted in JSONL, ready to be saved to a file.
}
\description{
This function transforms a list of prompts generated by
\code{\link[=build_prompts_from_files]{build_prompts_from_files()}} into the appropriate JSONL format required for
the OpenAI Batch API.
}
