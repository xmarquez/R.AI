% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llamafile.R
\name{llamafile_single_request}
\alias{llamafile_single_request}
\title{Send a Single Request to a Llamafile Local API}
\usage{
llamafile_single_request(
  prompt,
  model = "LLaMA_CPP",
  max_retries = 10,
  temperature = 0.2,
  max_tokens = 300,
  json_mode = FALSE,
  content_extraction_fun,
  pause_cap = 1200,
  quiet = FALSE,
  ...
)
}
\arguments{
\item{prompt}{A list containing the prompt message(s). Typically this is the
output of \code{\link[=prompt]{prompt()}}.}

\item{model}{The model to use for generating responses. The name is typically
automatically extracted from the llamafile when it is started via
\code{\link[=start_llamafile]{start_llamafile()}} or \code{\link[=call_api]{call_api()}}, and it doesn't much matter. The
default is \code{"LlaMA_CPP"}}

\item{max_retries}{The maximum number of retry attempts in case of request
failures. Defaults to 10.}

\item{temperature}{A numeric value between 0 and 1 that controls the
randomness of the response. Higher values make the output more random.
Defaults to 0.2.}

\item{max_tokens}{The maximum number of tokens to include in the response.
Defaults to 300.}

\item{json_mode}{A logical value indicating whether the response should be
parsed as JSON. Defaults to \code{FALSE}.}

\item{content_extraction_fun}{A function to extract the desired content from
the API response. If not provided, a default extraction function is used
depending on the value of \code{json_mode}.}

\item{pause_cap}{A numeric value representing the maximum pause duration (in
seconds) between retries. Defaults to 1200.}

\item{quiet}{A logical value indicating whether the function should suppress
messages during retries. Defaults to \code{FALSE}.}

\item{...}{Other parameters passed on to the local llamafile server.
Currently ignored.}
}
\value{
A \code{\link[=tibble]{tibble()}} with a \code{response} column and usage information.
}
\description{
This function sends a single prompt to a locally running Llamafile instance
using the OpenAI-compatible API endpoint. It allows users to customize
parameters such as model, number of candidates, and more. It also handles
retries and extracts the relevant response content.
}
\details{
You must start the model manually or via \code{\link[=start_llamafile]{start_llamafile()}} before
sending a request (if it is not already running on port 8080). For the
moment only one running model can be accessed at a time; even if you have
enough memory to run more than one llamafile simultaneously, only the first
one started on port 8080 will be accessible (so, no model running in 8081
will be visible to this function).
}
\seealso{
Other llamafile: 
\code{\link{is_llamafile_running}()},
\code{\link{kill_llamafile}()},
\code{\link{start_llamafile}()},
\code{\link{which_llamafile_running}()}

Other single message: 
\code{\link{claude_single_request}()},
\code{\link{groq_single_request}()},
\code{\link{mistral_single_request}()},
\code{\link{openai_single_request}()}
}
\concept{llamafile}
\concept{single message}
