#' Call Language Model API
#'
#' This function sends prompts to various language model APIs and retrieves
#' responses. It supports multiple APIs including Groq, OpenAI, Claude
#' (Anthropic), and Gemini.
#'
#' @param prompts A list of prompts to send to the API. The class of this object
#'   should match one of the supported APIs - "groq", "claude" (anthropic),
#'   "openai", or "gemini". These prompts are generated by
#'   [build_prompts_from_files()], which adds the required class.
#' @param model A string specifying the model to use. Must be a valid model for
#'   the API specified in the class of `prompts`. If not specified, the cheapest
#'   model for the API is retrieved via [get_default_model()].
#' @param prompt_name An optional string specifying the type of prompt. If not
#'   provided, it defaults to "json" if json_mode is TRUE, otherwise "default".
#'   The prompt name determines the response validation and content validation
#'   functions; thus a prompt name of "clean_data" will assume there is a
#'   function called {api}_clean_data_response_validation and a function called
#'   {api}_clean_data_content_extraction that perform the validation of your
#'   response and extraction of the content.
#' @param max_retries An integer specifying the maximum number of retries for
#'   failed API calls. Default is 10.
#' @param temperature A numeric value between 0 and 1 controlling the randomness
#'   of the model's output. Lower values make the output more deterministic.
#'   Default is 0.2.
#' @param max_tokens An integer specifying the maximum number of tokens in the
#'   model's response. Default is 300.
#' @param json_mode A logical value indicating whether the response should be in
#'   JSON format. Default is TRUE.
#' @param system An optional system message (for Claude only).
#' @param pause_cap Maximum number of seconds to wait before retrying a request.
#'   Default is 1200s, about 20 minutes.
#' @param llamafile_path A path to a local
#'   [llamafile](https://github.com/Mozilla-Ocho/llamafile), if using local
#'   inference.
#' @param log Whether to provide informative messages for a [crew] log (when
#'   using [target]), or print them to the screen.
#'
#' @return A tibble containing the API responses and usage information,
#'   including:
#'   - prompt_id: Identifier for each prompt
#'   - api: The API used
#'   - model: The model used
#'   - input_tokens: Number of tokens in the input
#'   - output_tokens: Number of tokens in the output
#'   - total_tokens: Total number of tokens used
#'   - input_cost: Cost for input tokens
#'   - output_cost: Cost for output tokens
#'   - total_cost: Total cost of the API call
#'   - response: The model's response
#'
#' @details This function is implemented as a generic with methods for different
#'   APIs:
#' - call_api.groq
#' - call_api.claude
#' - call_api.openai
#' - call_api.gemini
#' - call_api.mistral
#' - call_api.local_llamafile (for a local [llamafile](https://github.com/Mozilla-Ocho/llamafile))
#'
#'   Each method handles API-specific details such as endpoint URLs,
#'   authentication, and response parsing.
#'
#'   The function includes built-in error handling and retries for failed API
#'   calls. It also implements rate limiting to prevent exceeding API usage
#'   limits.
#'
#'   For JSON responses, the function includes validation and parsing of the
#'   JSON content.
#'
#' @examples
#' \dontrun{
#' # Assuming 'prompts' is a list of prompts with class 'openai'
#' responses <- call_api(prompts,
#'                       api = "openai",
#'                       model = "gpt-3.5-turbo",
#'                       temperature = 0.7,
#'                       max_tokens = 500)
#' }
#'
#' @seealso [build_prompts_from_files()] for creating prompts to use with this
#'   function.
#'
#' @export
call_api <- function(prompts,
                     model,
                     prompt_name,
                     ...) {

  UseMethod("call_api", prompts)

}

#' @export
call_api.default <- function(prompts,
                             model,
                             prompt_name,
                             n_candidates,
                             max_retries,
                             temperature,
                             max_tokens,
                             json_mode,
                             system,
                             pause_cap,
                             single_request_fun,
                             response_validation_fun,
                             content_extraction_fun,
                             llamafile_path,
                             log) {

  ids <- names(prompts)

  responses <- tibble()

  prompt_set <- digest::digest(ids)

  if(!missing(llamafile_path)) {
    if(!is_llamafile_running()) {
      start_llamafile(llamafile_path = llamafile_path)
    }
  }

  for(i in 1:length(prompts)) {
    if(log) {
      if(targets::tar_active()) {
        # Prints log message in a format that can be read by crew_log_raw and crew_log_summary if using with the targets package
        message(glue::glue("___LOG___|{lubridate::now()}|{class(prompts)[1]}|{prompt_name}|{prompt_set}|{model}|{i}|{length(prompts)}|___LOG___"))
      } else {
        # Prints log message to screen
        message(glue::glue("{lubridate::now()}: Now using {class(prompts)[1]} api model {model} to process prompt `{prompt_name}`, {i} of {length(prompts)}"))
      }
    }
    response <- do.call(single_request_fun,
                        list(prompt = prompts[[i]],
                             model = model,
                             n_candidates = n_candidates,
                             max_retries = max_retries,
                             temperature = temperature,
                             max_tokens = max_tokens,
                             json_mode = json_mode,
                             system = system,
                             response_validation_fun = response_validation_fun,
                             content_extraction_fun = content_extraction_fun,
                             pause_cap = pause_cap,
                             quiet = !log))
    response$id <- ids[i]
    responses <- dplyr::bind_rows(responses, response)
  }

  responses |>
    dplyr::mutate(api = class(prompts)[1]) |>
    dplyr::relocate(dplyr::all_of(c("id", "api", "model", "response")),
                    .before = dplyr::everything())

}

#' @export
call_api.groq <- function(prompts,
                          model,
                          prompt_name,
                          n_candidates = 1,
                          max_retries = 10,
                          temperature = 0.2,
                          max_tokens = 300,
                          json_mode = TRUE,
                          system = NULL,
                          pause_cap = 1200,
                          log = TRUE) {

  if(missing(model)) {
    model <- get_default_model(class(prompts)[1])
  }

  if(missing(prompt_name)) {
    if(json_mode) {
      prompt_name <- "json"
    } else {
      prompt_name <- "default"
    }
  }

  validate_args_call_api(prompts = prompts,
                         model = model,
                         prompt_name = prompt_name,
                         n_candidates = n_candidates,
                         max_retries = max_retries,
                         temperature = temperature,
                         max_tokens = max_tokens,
                         json_mode = json_mode,
                         system = system,
                         pause_cap = pause_cap,
                         log = log)

  single_request_fun <- paste("groq_single_request")
  response_validation_fun <- paste("groq", prompt_name, "response_validation", sep = "_")
  content_extraction_fun <- paste("groq", prompt_name, "content_extraction", sep = "_")

  NextMethod(prompts,
             model = model,
             prompt_name = prompt_name,
             max_retries = max_retries,
             n_candidates = n_candidates,
             temperature = temperature,
             max_tokens = max_tokens,
             json_mode = json_mode,
             system = system,
             pause_cap = pause_cap,
             response_validation_fun = response_validation_fun,
             content_extraction_fun = content_extraction_fun,
             single_request_fun = single_request_fun,
             log = log)

}


#' @export
call_api.claude <- function(prompts,
                            model,
                            prompt_name,
                            n_candidates = 1,
                            max_retries = 10,
                            temperature = 0.2,
                            max_tokens = 300,
                            json_mode = TRUE,
                            system = NULL,
                            pause_cap = 1200,
                            log = TRUE) {

  if(missing(model)) {
    model <- get_default_model(class(prompts)[1])
  }

  if(missing(prompt_name)) {
    if(json_mode) {
      prompt_name <- "json"
    } else {
      prompt_name <- "default"
    }
  }

  validate_args_call_api(prompts = prompts,
                         model = model,
                         prompt_name = prompt_name,
                         n_candidates = n_candidates,
                         max_retries = max_retries,
                         temperature = temperature,
                         max_tokens = max_tokens,
                         json_mode = json_mode,
                         system = system,
                         pause_cap = pause_cap,
                         log = log)

  single_request_fun <- paste("claude_single_request")
  response_validation_fun <- paste("claude", prompt_name, "response_validation", sep = "_")
  content_extraction_fun <- paste("claude", prompt_name, "content_extraction", sep = "_")

  NextMethod(prompts,
             model = model,
             prompt_name = prompt_name,
             max_retries = max_retries,
             n_candidates = n_candidates,
             temperature = temperature,
             max_tokens = max_tokens,
             json_mode = json_mode,
             system = system,
             pause_cap = pause_cap,
             response_validation_fun = response_validation_fun,
             content_extraction_fun = content_extraction_fun,
             single_request_fun = single_request_fun,
             log = log)

}

#' @export
call_api.openai <- function(prompts,
                            model,
                            prompt_name,
                            n_candidates = 1,
                            max_retries = 10,
                            temperature = 0.2,
                            max_tokens = 300,
                            json_mode = TRUE,
                            system = NULL,
                            pause_cap = 1200,
                            log = TRUE) {

  if(missing(model)) {
    model <- get_default_model(class(prompts)[1])
  }

  if(missing(prompt_name)) {
    if(json_mode) {
      prompt_name <- "json"
    } else {
      prompt_name <- "default"
    }
  }

  validate_args_call_api(prompts = prompts,
                         model = model,
                         prompt_name = prompt_name,
                         n_candidates = n_candidates,
                         max_retries = max_retries,
                         temperature = temperature,
                         max_tokens = max_tokens,
                         json_mode = json_mode,
                         system = system,
                         pause_cap = pause_cap,
                         log = log)

  single_request_fun <- paste("openai_single_request")
  response_validation_fun <- paste("openai", prompt_name, "response_validation", sep = "_")
  content_extraction_fun <- paste("openai", prompt_name, "content_extraction", sep = "_")

  NextMethod(prompts,
             model = model,
             prompt_name = prompt_name,
             max_retries = max_retries,
             n_candidates = n_candidates,
             temperature = temperature,
             max_tokens = max_tokens,
             json_mode = json_mode,
             system = system,
             response_validation_fun = response_validation_fun,
             content_extraction_fun = content_extraction_fun,
             single_request_fun = single_request_fun,
             pause_cap = pause_cap,
             log = log)

}

#' @export
call_api.gemini <- function(prompts,
                            model,
                            prompt_name,
                            n_candidates = 1,
                            max_retries = 10,
                            temperature = 0.2,
                            max_tokens = 300,
                            json_mode = TRUE,
                            system = NULL,
                            pause_cap = 1200,
                            log = TRUE) {

  if(missing(model)) {
    model <- get_default_model(class(prompts)[1])
  }

  if(missing(prompt_name)) {
    if(json_mode) {
      prompt_name <- "json"
    } else {
      prompt_name <- "default"
    }
  }

  validate_args_call_api(prompts = prompts,
                         model = model,
                         prompt_name = prompt_name,
                         n_candidates = n_candidates,
                         max_retries = max_retries,
                         temperature = temperature,
                         max_tokens = max_tokens,
                         json_mode = json_mode,
                         system = system,
                         pause_cap = pause_cap,
                         log = log)

  single_request_fun <- paste("gemini_single_request")
  response_validation_fun <- paste("gemini", prompt_name, "response_validation", sep = "_")
  content_extraction_fun <- paste("gemini", prompt_name, "content_extraction", sep = "_")

  NextMethod(prompts,
             model = model,
             prompt_name = prompt_name,
             max_retries = max_retries,
             n_candidates = n_candidates,
             temperature = temperature,
             max_tokens = max_tokens,
             json_mode = json_mode,
             system = system,
             response_validation_fun = response_validation_fun,
             content_extraction_fun = content_extraction_fun,
             single_request_fun = single_request_fun,
             pause_cap = pause_cap,
             log = log)


}

#' @export
call_api.local_llamafile <- function(prompts,
                           model,
                           prompt_name,
                           n_candidates = 1,
                           max_retries = 10,
                           temperature = 0.2,
                           max_tokens = 300,
                           json_mode = TRUE,
                           system = NULL,
                           pause_cap = 1200,
                           llamafile_path,
                           log = TRUE) {

  if (missing(model)) {
    model <- "LLaMA_CPP"
  }

  if (missing(prompt_name)) {
    if (json_mode) {
      prompt_name <- "json"
    } else {
      prompt_name <- "default"
    }
  }

  validate_args_call_api(prompts = prompts,
                         model = model,
                         prompt_name = prompt_name,
                         n_candidates = n_candidates,
                         max_retries = max_retries,
                         temperature = temperature,
                         max_tokens = max_tokens,
                         json_mode = json_mode,
                         system = system,
                         pause_cap = pause_cap,
                         log = log,
                         llamafile_path = llamafile_path)

  single_request_fun <- "llamafile_single_request"
  response_validation_fun <- paste("llamafile", prompt_name, "response_validation", sep = "_")
  content_extraction_fun <- paste("llamafile", prompt_name, "content_extraction", sep = "_")

  NextMethod(prompts,
             model = model,
             prompt_name = prompt_name,
             max_retries = max_retries,
             n_candidates = n_candidates,
             temperature = temperature,
             max_tokens = max_tokens,
             json_mode = json_mode,
             system = system,
             response_validation_fun = response_validation_fun,
             content_extraction_fun = content_extraction_fun,
             single_request_fun = single_request_fun,
             llamafile_path = llamafile_path,
             pause_cap = pause_cap,
             log = log)
}


default_json_content_extraction <- function(json_string) {
  json_string |>
    purrr::map(default_json_content_cleaning) |>
    purrr::map(jsonlite::fromJSON) |>
    purrr::map(dplyr::as_tibble) |>
    purrr::list_rbind()
}

default_json_content_cleaning <- function(json_string) {
  json_string |>
    stringr::str_remove("```$") |>
    stringr::str_remove("```json( )?")  |>
    stringr::str_replace(stringr::regex("\\}\n.+", dotall = TRUE), "}")|>
    stringr::str_remove("<\\|eot_id\\|>$")
}

retry_response <- function(base_url,
                           api_key,
                           response_format,
                           body,
                           max_retries,
                           pause_cap,
                           quiet) {
  res <- httr::RETRY(
    verb = "POST",
    url = base_url,
    config = httr::add_headers("Authorization" = paste("Bearer", api_key),
                               "content-type" = "application/json",
                               "response_format" = response_format),
    body = body,
    encode = "json",
    times = max_retries,
    pause_base = 1,
    pause_cap = pause_cap,
    quiet = quiet
  )

  res

}
