% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cerebras-api.R, R/claude-api.R, R/gemini-api.R,
%   R/generics.R, R/groq-api.R, R/llama-cpp-api.R, R/mistral-api.R,
%   R/ollama-api.R, R/openai-api.R
\name{chat.cerebras_list}
\alias{chat.cerebras_list}
\alias{chat.claude_list}
\alias{chat.gemini_list}
\alias{chat}
\alias{chat.groq_list}
\alias{chat.llama_cpp_list}
\alias{chat.mistral_list}
\alias{chat.ollama_list}
\alias{chat.openai_list}
\title{Create a Chat Completion}
\usage{
\method{chat}{cerebras_list}(
  messages,
  model = get_default_model("cerebras"),
  max_tokens = NULL,
  json_mode = FALSE,
  seed = NULL,
  stop = NULL,
  temperature = 0.2,
  top_p = NULL,
  tools = NULL,
  tool_choice = NULL,
  user = NULL,
  quiet = FALSE
)

\method{chat}{claude_list}(
  messages,
  model = get_default_model("claude"),
  max_retries = 3,
  temperature = 0.2,
  max_tokens = 300,
  system = NULL,
  stop_sequences = NULL,
  top_k = NULL,
  top_p = NULL,
  tool_choice = NULL,
  tools = NULL,
  quiet = FALSE
)

\method{chat}{gemini_list}(
  messages,
  model = get_default_model("gemini"),
  use_cache_name = NULL,
  max_retries = 3,
  temperature = 0.2,
  max_tokens = 300,
  json_mode = FALSE,
  system = NULL,
  ttl = "300s",
  quiet = FALSE
)

chat(messages, ...)

\method{chat}{groq_list}(
  messages,
  model = get_default_model("groq"),
  frequency_penalty = NULL,
  max_tokens = NULL,
  n = 1,
  presence_penalty = NULL,
  response_format = NULL,
  seed = NULL,
  stop = NULL,
  temperature = 0.2,
  tools = NULL,
  tool_choice = NULL,
  parallel_tool_calls = NULL,
  user = NULL,
  json_mode = FALSE,
  max_retries = 3,
  quiet = FALSE
)

\method{chat}{llama_cpp_list}(messages, temperature = 0.8, stop = NULL, ...)

\method{chat}{mistral_list}(
  messages,
  model = get_default_model("mistral"),
  n = 1,
  max_retries = 3,
  temperature = 0.2,
  top_p = 1,
  max_tokens = 300,
  json_mode = FALSE,
  stop = NULL,
  random_seed = NULL,
  presence_penalty = 0,
  frequency_penalty = 0,
  quiet = FALSE
)

\method{chat}{ollama_list}(
  messages,
  model,
  tools = NULL,
  format = NULL,
  options = NULL,
  stream = FALSE,
  keep_alive = "5m",
  ...
)

\method{chat}{openai_list}(
  messages,
  model = get_default_model("openai"),
  store = NULL,
  metadata = NULL,
  frequency_penalty = NULL,
  logit_bias = NULL,
  logprobs = NULL,
  top_logprobs = NULL,
  max_tokens = NULL,
  max_completion_tokens = NULL,
  n = 1,
  modalities = NULL,
  prediction = NULL,
  audio = NULL,
  presence_penalty = NULL,
  response_format = NULL,
  seed = NULL,
  service_tier = NULL,
  stop = NULL,
  temperature = 0.2,
  top_p = NULL,
  tools = NULL,
  tool_choice = NULL,
  parallel_tool_calls = NULL,
  user = NULL,
  json_mode = FALSE,
  max_retries = 3,
  quiet = FALSE
)
}
\arguments{
\item{messages}{A list of user and system messages in the correct format for
the intended API. This is typically produced by \code{\link[=format_chat]{format_chat()}}, with class
\code{c("{api}_list", "list")}. The class should match the backend API.}

\item{model}{A string specifying the ID of the model to use. Check the
documentation for your chosen API for details or use \code{\link[=list_models]{list_models()}} to
find available models. The default is typically set with
\code{\link[=get_default_model]{get_default_model()}}. These are usually the cheapest good models for each
API as of the time of writing this documentation (2024-12-24):

\if{html}{\out{<div class="sourceCode r">}}\preformatted{preferred_models[,1:2]
#> # A tibble: 6 x 2
#>   api      cheapest               
#>   <chr>    <chr>                  
#> 1 openai   gpt-4o-mini            
#> 2 claude   claude-3-haiku-20240307
#> 3 mistral  ministral-3b-latest    
#> 4 groq     llama-3.1-8b-instant   
#> 5 cerebras llama3.1-8b            
#> 6 gemini   gemini-1.5-flash-latest
}\if{html}{\out{</div>}}}

\item{max_tokens}{Integer or \code{NULL}. The maximum number of tokens to
generate. For Openai, this is deprecated in favor of
\code{max_completion_tokens}. Not compatible with o1 series models.}

\item{json_mode}{Logical, defaults to \code{FALSE}. If \code{TRUE}, sets
\code{response_format} to produce JSON output. Make sure to provide proper
system/user instructions to the model to produce valid JSON.}

\item{seed}{Integer or \code{NULL}, defaults to \code{NULL}. Attempts deterministic
sampling if specified. Determinism not guaranteed.}

\item{stop}{Character vector, single string, or \code{NULL}, defaults to \code{NULL}.
Up to 4 sequences at which the API will stop generating further tokens.}

\item{temperature}{Numeric, defaults to 0.2. Controls the randomness of the
sampling. Higher values (closer to 1) increase randomness, while lower
values (closer to 0) make outputs more deterministic.}

\item{top_p}{Numeric or \code{NULL}, defaults to 1. Another way of controlling
randomness via nucleus sampling. Only the tokens comprising the \code{top_p}
probability mass are considered.}

\item{tools}{List or \code{NULL}, specifying a list of tools (functions) the model
may call.}

\item{tool_choice}{Controls which (if any) tool is called by the model.
\itemize{
\item For Anthropic: List or \code{NULL}. Specifies how the model should use tools. For
example, \code{list(name = "get_stock_price", description = "Get stock price",   input_schema = ...)}.
\item For Openai String or list or \code{NULL}. "none" means no
tool is called. "auto" means the model can choose. A named list
\verb{list(type="function", function=list(name="my_function"))} would force the
model to call that tool. Defaults to \code{NULL}.
}}

\item{user}{String or \code{NULL}, defaults to \code{NULL}. A unique identifier
representing the end-user.}

\item{quiet}{Logical, defaults to \code{FALSE}. If \code{TRUE}, suppress messages
during retries.}

\item{max_retries}{Integer, defaults to 3. Maximum number of retries if the
API request fails.}

\item{system}{Character or \code{NULL}. Optional system-level prompt providing
context and instructions (e.g., specifying a goal or role). If \code{NULL}, the
function will try to find it in the \code{"system"} attribute, if any, of
\code{messages}.}

\item{stop_sequences}{Character vector or \code{NULL}. Custom text sequences that
will cause the model to stop generating. Defaults to \code{NULL}.}

\item{top_k}{Integer or \code{NULL}, defaults to \code{NULL}. Limits sampling to the
top K tokens for each step.}

\item{use_cache_name}{\strong{Gemini Only}. If provided, the API will attempt to
use cached contents using the specific cache name. The cache name is
returned in the response if the messages have a cache attribute created by
\code{\link[=format_chat]{format_chat()}}; it is not user-created. Typically, you will call \code{\link[=chat]{chat()}},
extract the cache name, and call \code{\link[=chat]{chat()}} again with a new query and the
paramter \code{use_cache_name} set to the \code{cache_name} field of the first
response.}

\item{ttl}{\strong{Gemini only}. "Time to live" for cached messages. Defaults to
300s.}

\item{...}{Additional arguments passed to methods.}

\item{frequency_penalty}{Numeric or \code{NULL}. A value between -2.0 and 2.0 that
penalizes new tokens based on their existing frequency in the text so far,
reducing the likelihood of repeating the same line verbatim. Defaults to
\code{NULL} (no penalty sent).}

\item{n}{Integer or \code{NULL}, defaults to 1. How many chat completion choices
to generate for each input message. This is only implemented in the Openai
and Mistral APIs for now.}

\item{presence_penalty}{Numeric or \code{NULL}. A value between -2.0 and 2.0 that
penalizes new tokens if they appear in the text so far, encouraging the
model to talk about new topics. Defaults to \code{NULL} (no penalty).}

\item{response_format}{List or \code{NULL}. An object specifying the format the
model must output. For example, \code{list(type = "json_object")} ensures the
model produces JSON.}

\item{parallel_tool_calls}{Logical, defaults to \code{NULL}. Whether to enable
parallel tool calls.}

\item{options}{\strong{Ollama only.} Model parameters such as \code{temperature}, in a
named list. These are documented
\href{https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values}{here}.}

\item{keep_alive}{\strong{Ollama only.} Duration to keep the model in memory after
the request. Defaults to 5m.}

\item{store}{\strong{Openai only}. Logical or \code{NULL}. Whether or not to store the output of this
chat completion request for use in OpenAI's model distillation or evals
products. Defaults to \code{NULL} (meaning don't send the parameter unless
needed).}

\item{metadata}{\strong{Openai only}. A named list or \code{NULL}, containing developer-defined tags and
values used for filtering completions in the dashboard.}

\item{logit_bias}{A named list or \code{NULL}, mapping tokens (by their token ID)
to a numeric bias value from -100 to 100. Adjusts the likelihood of
specified tokens appearing in the completion. Defaults to \code{NULL} (no bias).}

\item{logprobs}{Logical or \code{NULL}, defaults to \code{NULL}. Whether to return log
probabilities of the output tokens. If \code{TRUE}, returns log probabilities of
each output token in the message content.}

\item{top_logprobs}{Integer or \code{NULL}, defaults to \code{NULL}. Number of top
tokens to return with log probabilities at each token position. Must be
used with \code{logprobs = TRUE}.}

\item{max_completion_tokens}{\strong{Openai only}. Integer or \code{NULL}, defaults to
\code{NULL}. The upper bound for the number of tokens generated for the
completion, including visible and reasoning tokens.}

\item{modalities}{Character vector or \code{NULL}, specifying desired output
modalities. Typically \code{c("text")} or for certain models \code{c("text", "audio")}.}

\item{prediction}{\strong{Openai only}. List or \code{NULL}. Configuration for a predicted output,
improving response times if large parts of the response are already known.}

\item{audio}{\strong{Openai only}. List or \code{NULL}. Parameters for audio output. Required when audio
output is requested.}

\item{service_tier}{\strong{Openai only}. Character or \code{NULL}, defaults to \code{NULL}.
Specifies the latency tier to use. Options: "auto", "default".}

\item{message}{The message(s) to create a completion for.}

\item{`...`:}{Additional arguments passed to backend methods.}
}
\value{
A response object from the chosen backend. All objects returned by
chat methods should have class \verb{\{api\}_chat}, so that they can be dispatched
to \code{\link[=get_message]{get_message()}}, \code{\link[=get_content]{get_content()}}, and other methods.

An object of class \code{"openai_chat"} containing the response from the
OpenAI API.
}
\description{
A generic function for creating chat completions. The default method
dispatches to different registered backends, such as OpenAI.
}
\section{Supported Backends}{

\itemize{
\item OpenAI: Dispatches to the \href{https://platform.openai.com/docs/api-reference/chat}{OpenAI Chat API}.
\item Gemini: Dispatches to the \href{https://ai.google.dev/api/generate-content}{Gemini Chat API}.
\item Claude: Dispatches to the \href{https://docs.anthropic.com/en/api/messages}{Anthropic Claude API}.
\item Mistral: Dispatches to the \href{https://docs.mistral.ai/api/#tag/chat}{Mistral Chat API}.
\item Groq: Dispatches to the \href{https://console.groq.com/docs/api-reference#chat}{Groq Chat API}.
\item Cerebras: Dispatches to the \href{https://inference-docs.cerebras.ai/api-reference/chat-completions}{Cerebras Chat API}.
\item Llama.cpp: Dispatches to a local LlamaFile instance or a Llama.CPP server running a GGUF formatted model.
\item Ollama: Dispatches to a local model running on an \href{https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion}{Ollama local server}.
}
}

\section{Environment Variables}{


Each backend, except for local backends (llama_cpp and ollama), requires an
API key to be set in the appropriate environment variable. Use
\code{\link[=get_available_apis]{get_available_apis()}} to check if a key is set. Most keys should be of the
form \code{API_NAME_API_KEY}, except for Claude, where the key should be
\code{ANTHROPIC_API_KEY}. Set key by using Sys.setenv("API_NAME_API_KEY" =
"YOUR_API_KEY"), or saving it in your .Renviron file.
}

\examples{
\dontrun{
  response <- chat.claude_list(
    messages = list(
      list(role = "user", content = "What is the capital of France?")
    ),
    model = "claude-3-haiku-20240307"
  )
  print(response)
}

}
\seealso{
\url{https://inference-docs.cerebras.ai/api-reference/}

\url{https://docs.anthropic.com/en/api/messages}

\url{https://console.groq.com/docs/api-reference#chat-create}

\url{https://github.com/ggerganov/llama.cpp/tree/master/examples/server#post-v1chatcompletions-openai-compatible-chat-completions-api}

\url{https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion}

\url{https://platform.openai.com/docs/api-reference/chat/create}
}
