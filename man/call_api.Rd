% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/api.R
\name{call_api}
\alias{call_api}
\title{Call Language Model API}
\usage{
call_api(
  prompts,
  model,
  prompt_name,
  rate_limit,
  max_retries,
  temperature,
  max_tokens,
  json_mode,
  system
)
}
\arguments{
\item{prompts}{A list of prompts to send to the API. The class of this object
should match one of the supported APIs - "groq", "claude" (anthropic),
"openai", or "gemini". These prompts are generated by
\code{\link[=build_prompts_from_files]{build_prompts_from_files()}}, which adds the required class.}

\item{model}{A string specifying the model to use. Must be a valid model for
the API specified in the class of \code{prompts}. If not specified, the cheapest
model for the API is retrieved via \code{\link[=get_default_model]{get_default_model()}}.}

\item{prompt_name}{An optional string specifying the type of prompt. If not
provided, it defaults to "json" if json_mode is TRUE, otherwise "default".
The prompt name determines the response validation and content validation
functions; thus a prompt name of "clean_data" will assume there is a
function called {api}_clean_data_response_validation and a function called
{api}_clean_data_content_extraction that perform the validation of your
response and extraction of the content.}

\item{rate_limit}{A numeric value specifying the minimum time (in seconds)
between API calls. Default is 2.}

\item{max_retries}{An integer specifying the maximum number of retries for
failed API calls. Default is 10.}

\item{temperature}{A numeric value between 0 and 1 controlling the randomness
of the model's output. Lower values make the output more deterministic.
Default is 0.2.}

\item{max_tokens}{An integer specifying the maximum number of tokens in the
model's response. Default is 300.}

\item{json_mode}{A logical value indicating whether the response should be in
JSON format. Default is TRUE.}

\item{system}{An optional system message (for Claude only).}
}
\value{
A tibble containing the API responses and usage information,
including:
\itemize{
\item prompt_id: Identifier for each prompt
\item api: The API used
\item model: The model used
\item input_tokens: Number of tokens in the input
\item output_tokens: Number of tokens in the output
\item total_tokens: Total number of tokens used
\item input_cost: Cost for input tokens
\item output_cost: Cost for output tokens
\item total_cost: Total cost of the API call
\item response: The model's response
}
}
\description{
This function sends prompts to various language model APIs and retrieves
responses. It supports multiple APIs including Groq, OpenAI, Claude
(Anthropic), and Gemini.
}
\details{
This function is implemented as a generic with methods for different
APIs:
\itemize{
\item call_api.groq
\item call_api.claude
\item call_api.openai
\item call_api.gemini

Each method handles API-specific details such as endpoint URLs,
authentication, and response parsing.

The function includes built-in error handling and retries for failed API
calls. It also implements rate limiting to prevent exceeding API usage
limits.

For JSON responses, the function includes validation and parsing of the
JSON content.
}
}
\examples{
\dontrun{
# Assuming 'prompts' is a list of prompts with class 'openai'
responses <- call_api(prompts,
                      api = "openai",
                      model = "gpt-3.5-turbo",
                      temperature = 0.7,
                      max_tokens = 500)
}

}
\seealso{
\code{\link[=build_prompts_from_files]{build_prompts_from_files()}} for creating prompts to use with this
function.
}
