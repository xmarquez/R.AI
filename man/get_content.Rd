% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/get-content-methods.R
\name{get_content}
\alias{get_content}
\alias{get_content.openai_chat}
\alias{get_content.cerebras_chat}
\alias{get_content.claude_chat}
\alias{get_content.gemini_chat}
\alias{get_content.groq_chat}
\alias{get_content.llama_cpp_completion}
\alias{get_content.llama_cpp_chat}
\alias{get_content.mistral_chat}
\alias{get_content.ollama_completion}
\alias{get_content.ollama_chat}
\alias{get_content.cohere_chat}
\alias{get_content.deepseek_chat}
\alias{get_content.qwen_chat}
\alias{get_content.default}
\title{Extract Main Text Content from a Response}
\usage{
get_content(response)

\method{get_content}{openai_chat}(response)

\method{get_content}{cerebras_chat}(response)

\method{get_content}{claude_chat}(response)

\method{get_content}{gemini_chat}(response)

\method{get_content}{groq_chat}(response)

\method{get_content}{llama_cpp_completion}(response)

\method{get_content}{llama_cpp_chat}(response)

\method{get_content}{mistral_chat}(response)

\method{get_content}{ollama_completion}(response)

\method{get_content}{ollama_chat}(response)

\method{get_content}{cohere_chat}(response)

\method{get_content}{deepseek_chat}(response)

\method{get_content}{qwen_chat}(response)

\method{get_content}{default}(response)
}
\arguments{
\item{response}{A response object returned by a chat or completion method. Each
backend typically stores the generated text content in a slightly different place.}
}
\value{
A character vector. Usually one element per "choice" or "candidate"
that the backend produced. If a backend only ever returns one choice, then
you'll typically get a length-1 character vector.
}
\description{
A generic function that extracts the \strong{textual content} from various LLM
(Large Language Model) response objects (e.g., OpenAI, Gemini, Claude, Cerebras,
etc.). By default, it returns \strong{one character element per generated candidate} or choice.
}
\section{OpenAI}{

\itemize{
\item \code{get_content.openai_chat()} returns a character vector of length \code{n}, where \code{n}
is the number of choices (\code{response$choices}).
}
}

\section{Gemini}{

\itemize{
\item \code{get_content.gemini_chat()} returns \strong{one string per candidate} in
\code{response$candidates}. Each candidate’s text may be split into multiple
parts (\code{candidate$content$parts}). These parts are concatenated. If there
are multiple candidates, you get a vector of length > 1.
}
}

\section{Other Methods}{

\itemize{
\item \strong{Claude} (\code{claude_chat}): Returns the \code{text} from the first item in
\code{response$content}.
\item \strong{Cerebras} (\code{cerebras_chat}): Returns the \code{content} from the first
item in \code{response$choices}.
\item \strong{Groq} (\code{groq_chat}): Similar to OpenAI, returns \code{message$content} from
the first item in \code{response$choices}.
\item \strong{Llama.CPP} (\code{llama_cpp_completion}, \code{llama_cpp_chat}): May differ
slightly between “completion” and “chat” responses. For chat style,
returns a vector of choices (if multiple). For completion style,
typically a single string in \code{response$content}.
\item \strong{Mistral} (\code{mistral_chat}): Modeled after the same pattern; returns
multiple strings if the API returned multiple choices.
\item \strong{Ollama} (\code{ollama_completion}, \code{ollama_chat}): Depending on whether
it’s a chat or a completion, the text may be in different fields:
\code{response$message$content} or \code{response$response}.
}
}

\examples{
\dontrun{
# OpenAI example (may return multiple choices if n>1)
resp_oa <- chat.openai_list(messages, model="gpt-3.5-turbo")
texts_oa <- get_content(resp_oa)
print(texts_oa)

# Gemini example with multiple candidates
resp_gem <- chat.gemini_list(messages, model="gemini-v1", ...)
texts_gem <- get_content(resp_gem)
print(texts_gem) # Possibly a vector of multiple candidates

# Claude example
resp_claude <- chat.claude_list(messages)
text_claude <- get_content(resp_claude)
cat(text_claude)
}

}
