% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/get-usage-methods.R
\name{get_usage}
\alias{get_usage}
\alias{get_usage.default}
\alias{get_usage.claude_chat}
\alias{get_usage.gemini_chat}
\alias{get_usage.llama_cpp_completion}
\alias{get_usage.ollama_chat}
\alias{get_usage.ollama_completion}
\title{Retrieve Usage Information from a Chat or Completion Response}
\usage{
get_usage(response)

\method{get_usage}{default}(response)

\method{get_usage}{claude_chat}(response)

\method{get_usage}{gemini_chat}(response)

\method{get_usage}{llama_cpp_completion}(response)

\method{get_usage}{ollama_chat}(response)

\method{get_usage}{ollama_completion}(response)
}
\arguments{
\item{response}{A response object returned by a chat or completion call.
This object should contain usage metadata (e.g., \code{response$usage},
\code{response$usageMetadata}, or related fields), depending on the backend.}
}
\value{
A data frame (often a tibble) containing usage statistics. The exact
columns depend on the backend, but commonly include:
\itemize{
\item \code{input_tokens} (prompt tokens),
\item \code{output_tokens} (completion tokens),
\item \code{total_tokens},
\item \code{model},
\item other fields as provided by the specific backend.
}
}
\description{
A generic function to extract usage details (e.g., token counts) from response
objects returned by various language model (LLM) backends. Calling \code{get_usage()}
will return a standardized data frame (commonly a tibble) summarizing usage metrics
such as prompt tokens, completion tokens, total tokens, etc.
}
\section{Default Method}{

The default method (\code{get_usage.default()}) checks for a \code{response$usage} object
with fields like \code{prompt_tokens}, \code{completion_tokens}, and \code{total_tokens}.
If present, it also tries to parse any \code{prompt_tokens_details} or
\code{completion_tokens_details}.
}

\section{Claude (\code{claude_chat}) Method}{

The Claude usage method extracts usage from \code{response$usage} (including fields
like \code{input_tokens} and \code{output_tokens}). It also accounts for
\code{cache_creation_input_tokens} or \code{cache_read_input_tokens} if present.
}

\section{Gemini (\code{gemini_chat}) Method}{

The Gemini usage method reads usage data from the \code{response$usageMetadata} field,
renaming columns to \code{input_tokens}, \code{output_tokens}, and \code{total_tokens} as needed.
}

\section{Llama.CPP (\code{llama_cpp_completion}) Method}{

The Llama.CPP usage method extracts usage fields such as \code{tokens_cached},
\code{tokens_evaluated}, and \code{tokens_predicted}.
}

\section{Ollama (\code{ollama_chat} and \code{ollama_completion}) Methods}{

Ollama usage methods combine prompt eval counts and output eval counts (or tokens)
into a tibble. For completions, additional fields may be shown as returned
by the Ollama server.
}

\examples{
\dontrun{
## Claude example:
response_claude <- chat.claude_list(messages, model = "claude-v1")
usage_info <- get_usage(response_claude)
print(usage_info)

## Gemini example:
response_gemini <- chat.gemini_list(messages, model = "gemini-v1")
usage_info <- get_usage(response_gemini)
print(usage_info)
}

}
