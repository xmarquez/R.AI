% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/completion-methods.R
\name{completion}
\alias{completion}
\alias{completion.llama_cpp_character}
\alias{completion.ollama_character}
\title{Generate Text Completions}
\usage{
completion(prompt, ...)

\method{completion}{llama_cpp_character}(
  prompt,
  n_predict = -1,
  temperature = 0.8,
  top_k = 40,
  top_p = 0.95,
  stream = FALSE,
  ...
)

\method{completion}{ollama_character}(
  prompt,
  model,
  suffix = NULL,
  images = NULL,
  options = NULL,
  system = NULL,
  template = NULL,
  stream = FALSE,
  raw = FALSE,
  keep_alive = "5m",
  ...
)
}
\arguments{
\item{prompt}{Character string(s) serving as the prompt(s). Must be a
character vector of length one or more. The S3 method for each backend
may support different additional parameters.}

\item{...}{Additional arguments passed to the specific backend method.
For instance:
\itemize{
\item Llama.CPP: \code{n_predict}, \code{temperature}, \code{top_k}, \code{top_p}, \code{stream}
\item Ollama: \code{model}, \code{suffix}, \code{images}, \code{options}, \code{system}, etc.
}}
}
\value{
Each method returns a structure containing the completion text or a
list of tokens, depending on the backend. Typically, the object has a class
like \code{"llama_cpp_completion"} or \code{"ollama_completion"}.
}
\description{
A generic function to produce text completions from various backends.
Currently supports Llama.CPP and Ollama. By dispatching on the class of
\code{prompt} (e.g., \code{"llama_cpp_character"}, \code{"ollama_character"}), the
appropriate method is chosen.
}
\section{Examples}{

\dontrun{
# Example calling Llama.CPP:
prompt_llama <- "Tell me a story about dragons."
class(prompt_llama) <- c("llama_cpp_character", "character")
resp_llama <- completion(prompt_llama, n_predict=100, temperature=0.7)

# Example calling Ollama:
prompt_ollama <- "What are the main differences between cats and dogs?"
class(prompt_ollama) <- c("ollama_character", "character")
resp_ollama <- completion(prompt_ollama, model="my-cool-model")
}
}

