% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llama-cpp-api.R
\name{health.llama_cpp}
\alias{health.llama_cpp}
\alias{props.llama_cpp}
\alias{completion.llama_cpp_character}
\alias{tokenize.llama_cpp_character}
\alias{embed.llama_cpp_character}
\alias{detokenize.llama_cpp_tokenlist}
\title{Functions to interact with the llama.cpp server API}
\usage{
health.llama_cpp(...)

props.llama_cpp(...)

\method{completion}{llama_cpp_character}(
  prompt,
  n_predict = -1,
  temperature = 0.8,
  top_k = 40,
  top_p = 0.95,
  stream = FALSE,
  ...
)

\method{tokenize}{llama_cpp_character}(content, add_special = FALSE, with_pieces = FALSE, ...)

\method{embed}{llama_cpp_character}(content, ...)

\method{detokenize}{llama_cpp_tokenlist}(tokens, ...)
}
\arguments{
\item{...}{Additional arguments passed to the HTTP request.}

\item{prompt}{Text input for generation. Can be a vector of characters.}

\item{n_predict}{Maximum tokens to generate. Use -1 for unlimited.}

\item{temperature}{Sampling temperature.}

\item{top_k}{Top-k sampling.}

\item{top_p}{Top-p sampling.}

\item{stream}{Enable streaming mode.}

\item{content}{Text to generate embeddings for.}

\item{add_special}{Include special tokens (default FALSE).}

\item{with_pieces}{Return token pieces (default FALSE).}

\item{tokens}{A vector of token IDs to detokenize into text.}
}
\description{
See
\url{https://github.com/ggerganov/llama.cpp/tree/master/examples/server}
for more details.
}
