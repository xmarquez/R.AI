% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/chat-methods.R
\name{chat}
\alias{chat}
\alias{chat.openai_list}
\alias{chat.gemini_list}
\alias{chat.claude_list}
\alias{chat.mistral_list}
\alias{chat.cerebras_list}
\alias{chat.groq_list}
\alias{chat.llama_cpp_list}
\alias{chat.ollama_list}
\alias{chat.cohere_list}
\alias{chat.deepseek_list}
\title{Create a Chat Completion}
\usage{
chat(messages, model, temperature, max_tokens, quiet, ...)

\method{chat}{openai_list}(
  messages,
  model = get_default_model("openai"),
  temperature = 0.2,
  max_tokens = NULL,
  quiet = FALSE,
  ...
)

\method{chat}{gemini_list}(
  messages,
  model = get_default_model("gemini"),
  temperature = 0.2,
  max_tokens = 300,
  quiet = FALSE,
  ...
)

\method{chat}{claude_list}(
  messages,
  model = get_default_model("claude"),
  temperature = 0.2,
  max_tokens = 300,
  quiet = FALSE,
  ...
)

\method{chat}{mistral_list}(
  messages,
  model = get_default_model("mistral"),
  temperature = 0.2,
  max_tokens = 300,
  quiet = FALSE,
  ...
)

\method{chat}{cerebras_list}(
  messages,
  model = get_default_model("cerebras"),
  temperature = 0.2,
  max_tokens = NULL,
  quiet = FALSE,
  ...
)

\method{chat}{groq_list}(
  messages,
  model = get_default_model("groq"),
  temperature = 0.2,
  max_tokens = NULL,
  quiet = FALSE,
  ...
)

\method{chat}{llama_cpp_list}(
  messages,
  model = NULL,
  temperature = 0.8,
  max_tokens = NULL,
  quiet = FALSE,
  ...
)

\method{chat}{ollama_list}(messages, model, temperature = 0.2, max_tokens = 300, quiet = FALSE, ...)

\method{chat}{cohere_list}(
  messages,
  model = "command-r-plus-08-2024",
  temperature = 0.3,
  max_tokens = NULL,
  quiet = FALSE,
  ...
)

\method{chat}{deepseek_list}(
  messages,
  model = "deepseek-chat",
  temperature = 1,
  max_tokens = 2048,
  quiet = FALSE,
  ...
)
}
\arguments{
\item{messages}{A list of user and system messages in the correct format for
the intended backend. Often produced by helper functions like
\code{\link[=format_chat]{format_chat()}}.}

\item{model}{A string specifying the ID of the model to use. See the docs or
\code{\link[=list_models]{list_models()}} for available models. Each backend has different model
naming conventions.}

\item{temperature}{Numeric (default \code{0.2}). Controls randomness during
sampling. Values closer to \code{1} produce more random outputs; values closer
to \code{0} produce more deterministic outputs.}

\item{max_tokens}{Integer or \code{NULL}. The maximum number of tokens to generate.
Some backends (e.g., OpenAI) prefer a separate parameter called
\code{max_completion_tokens}. \strong{Note} that for newer OpenAI “o1” series models,
this is not supported. For backward compatibility, you can still pass
\code{max_tokens} to older OpenAI models, but it is effectively deprecated in
favor of \code{max_completion_tokens}.}

\item{quiet}{Logical (default \code{FALSE}). If \code{TRUE}, suppress messages (including
retry notifications).}

\item{...}{Additional arguments passed to the backend-specific method.}
}
\value{
A response object from the chosen backend (S3 class typically named
\verb{\{backend\}_chat}).
}
\description{
A generic S3-based interface for creating chat completions with multiple
backends (OpenAI, Gemini, Claude, Cerebras, Groq, Llama.cpp, Ollama). Each
backend has its own method, but all methods share the same formal signature.
}
\section{Supported Backends}{

\itemize{
\item \strong{OpenAI}: Dispatches to the \href{https://platform.openai.com/docs/api-reference/chat}{OpenAI Chat API}.
\item \strong{Gemini}: Dispatches to the \href{https://ai.google.dev/api/generate-content}{Gemini Chat API}.
\item \strong{Claude}: Dispatches to the \href{https://docs.anthropic.com/en/api/messages}{Anthropic Claude API}.
\item \strong{Cohere}: Dispatches to the \href{https://docs.cohere.com/v2/reference/chat}{Cohere Chat API}.
\item \strong{DeepSeek}: Dispatches to the \href{https://api.deepseek.com/chat/completions}{DeepSeek Chat API}.
\item \strong{Mistral}: Dispatches to the \href{https://docs.mistral.ai/api/#tag/chat}{Mistral Chat API}.
\item \strong{Groq}: Dispatches to the \href{https://console.groq.com/docs/api-reference#chat}{Groq Chat API}.
\item \strong{Cerebras}: Dispatches to the \href{https://inference-docs.cerebras.ai/api-reference/chat-completions}{Cerebras Chat API}.
\item \strong{Llama.cpp}: Dispatches to a local LlamaFile instance or an Llama.CPP server running a GGUF model.
\item \strong{Ollama}: Dispatches to a local model via the \href{https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion}{Ollama server}.
}
}

\section{Environment Variables}{

Each non-local backend requires an API key. For example, OpenAI uses
\code{OPENAI_API_KEY}, Gemini uses \code{GEMINI_API_KEY}, Claude uses
\code{ANTHROPIC_API_KEY}, etc. You can set them via \code{Sys.setenv()} or in
your \code{.Renviron}.
}

\section{Common Additional Parameters}{

The following are recognized by some or all backends. If a backend does not
support a parameter, it will be silently ignored.
\itemize{
\item \strong{\code{presence_penalty}} (Numeric or \code{NULL}): Used by OpenAI (and some others)
to penalize repeated content, promoting new topics.
\item \strong{\code{frequency_penalty}} (Numeric or \code{NULL}): Also primarily for OpenAI.
\item \strong{\code{top_p}} (Numeric or \code{NULL}): Probability mass threshold for nucleus
sampling.
\item \strong{\code{top_k}} (Integer or \code{NULL}): Typically for Claude, limiting sampling to
the top-k tokens.
\item \strong{\code{stop}} (Character vector/string or \code{NULL}): Custom sequences that cause
the model to stop.
\item \strong{\code{stop_sequences}} (Character vector or \code{NULL}): Claude-specific name for
stops.
\item \strong{\code{tools}, \code{tool_choice}} (List or \code{NULL}): Ad-hoc mechanism for
instructing a model to call external functions. Variation in how each API
implements these.
\item \strong{\code{parallel_tool_calls}} (Logical or \code{NULL}): For backends supporting
concurrent tool invocation.
\item \strong{\code{json_mode}} (Logical, default \code{FALSE}): If \code{TRUE}, requests and
responses favor JSON outputs. Supported by some backends (OpenAI, Groq,
Cerebras, etc.).
\item \strong{\code{user}} (String or \code{NULL}): Some backends let you specify a user ID.
\item \strong{\code{max_retries}} (Integer, default 3): How many times to retry on error.
\item \strong{\code{system}} (Character or \code{NULL}): A system-level prompt used by some
backends (e.g., Claude, Gemini) for extra context or instructions.
}
}

\section{Gemini-Specific Cache Parameters}{

\itemize{
\item \strong{\code{ttl}} (Gemini only): The “time to live” for cached content, default
\code{"300s"}.
\item \strong{\code{use_cache_name}} (Gemini only): If provided, tries to use existing
cached messages in caches with this name.
}
}

\examples{
\dontrun{
# OpenAI example
response <- chat.openai_list(
  messages = list(list(role="user", content="Hello!")),
  model = "gpt-3.5-turbo",
  temperature = 0.3
)

# Gemini example (with a system prompt):
response <- chat.gemini_list(
  messages = list(list(role="user", content="What's the weather?")),
  system = "You are a helpful weather assistant.",
  ttl = "300s"
)

# Claude example (with stop sequences):
response <- chat.claude_list(
  messages = list(list(role = "user", content = "Write a short poem.")),
  stop_sequences = c("##")
)
}

}
