% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/api.R
\name{call_batch_api}
\alias{call_batch_api}
\title{Call Language Model Batch API}
\usage{
call_batch_api(prompts, model, max_tokens, quiet)
}
\arguments{
\item{prompts}{A list of prompts to send to the API. The class of this object
should match one of the supported APIs: "mistral", "claude" (for the
Anthropic API), or "openai".}

\item{model}{A string specifying the model to use.}

\item{max_tokens}{The maximum number of output tokens for each item in the
batch. Defaults to 300.}

\item{quiet}{A logical value indicating whether the function should suppress
messages during retries. Defaults to FALSE.}

\item{prompt_name}{An optional string specifying the type of prompt.}
}
\value{
A list containing the response details, including batch ID,
processing status, and the URL for retrieving results (if ready).
}
\description{
This generic function sends batch requests to various language model APIs. It
supports multiple APIs, including OpenAI, Claude (Anthropic), and Mistral.
}
\details{
This function is implemented as a generic with methods for different
APIs:
\itemize{
\item call_batch_api.claude
\item call_batch_api.openai
\item call_batch_api.mistral

Each method handles API-specific details such as endpoint URLs,
authentication, and response parsing.
}
}
\seealso{
\code{\link[=build_prompts_from_files]{build_prompts_from_files()}} for creating prompts to use with this
function.
}
