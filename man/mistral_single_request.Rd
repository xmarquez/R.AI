% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mistral.R
\name{mistral_single_request}
\alias{mistral_single_request}
\title{Send a Single Request to the Mistral API}
\usage{
mistral_single_request(
  prompt,
  model,
  n_candidates = 1,
  max_retries = 10,
  temperature = 0.2,
  top_p = 1,
  max_tokens = 300,
  stop = NULL,
  random_seed = NULL,
  presence_penalty = 0,
  frequency_penalty = 0,
  safe_prompt = FALSE,
  content_extraction_fun,
  pause_cap = 1200,
  quiet = FALSE
)
}
\arguments{
\item{prompt}{A list containing the prompt message(s). Typically this is the
output of \code{\link[=prompt]{prompt()}}.}

\item{model}{A string specifying the model to use. Get available models with
\code{\link[=list_models]{list_models()}} or \code{\link[=get_available_models]{get_available_models()}}.}

\item{n_candidates}{The number of response candidates to generate. Defaults
to 1.}

\item{max_retries}{The maximum number of retry attempts in case of request
failures. Defaults to 10.}

\item{temperature}{A numeric value between 0 and 1 that controls the
randomness of the response. Higher values make the output more random.
Defaults to 0.2.}

\item{max_tokens}{The maximum number of tokens to include in the response.
Defaults to 300.}

\item{stop}{A character vector of stop sequences. The model will stop
generating further tokens when any of the specified sequences is
encountered. Defaults to \code{NULL}.}

\item{random_seed}{An integer for setting a seed for reproducibility.
Defaults to \code{NULL} (no fixed seed).}

\item{presence_penalty}{A numeric value that penalizes the presence of new
tokens. Values range from -2.0 to 2.0, with higher values encouraging the
generation of novel tokens. Defaults to 0.}

\item{frequency_penalty}{A numeric value that penalizes the frequency of
tokens that have already been generated. Values range from -2.0 to 2.0.
Defaults to 0.}

\item{safe_prompt}{A logical value indicating whether to validate the prompt
for safety before sending it to the API. Defaults to \code{FALSE}.}

\item{content_extraction_fun}{A function to extract the desired content from
the API response. If not provided, a default extraction function is used
depending on the value of \code{json_mode}.}

\item{pause_cap}{A numeric value representing the maximum pause duration (in
seconds) between retries. Defaults to 1200.}

\item{quiet}{A logical value indicating whether the function should suppress
messages during retries. Defaults to \code{FALSE}.}
}
\value{
A \code{\link[=tibble]{tibble()}} with a \code{response} column and usage information.
}
\description{
This function sends a single prompt to the Mistral API and retrieves the
response. It also handles retries and extracts the relevant response content.
}
\details{
See the Mistral documentation at
\url{https://docs.mistral.ai/capabilities/completion/}
for more.
}
\seealso{
Other single message: 
\code{\link{claude_single_request}()},
\code{\link{groq_single_request}()},
\code{\link{llamafile_single_request}()},
\code{\link{openai_single_request}()}
}
\concept{mistral}
\concept{single message}
