#' Call Language Model API
#'
#' This function sends prompts to various language model APIs and retrieves
#' responses. It supports multiple APIs including Groq, OpenAI, Claude
#' (Anthropic), and Gemini.
#'
#' @param prompts A list of prompts to send to the API. The class of this object
#'   should match one of the supported APIs - "groq", "claude" (anthropic),
#'   "openai", or "gemini". These prompts are generated by
#'   [build_prompts_from_files()], which adds the required class.
#' @param model A string specifying the model to use. Must be a valid model for
#'   the API specified in the class of `prompts`. If not specified, the cheapest
#'   model for the API is retrieved via [get_default_model()].
#' @param prompt_name An optional string specifying the type of prompt. If not
#'   provided, it defaults to "json" if json_mode is TRUE, otherwise "default".
#'   The prompt name determines the response validation and content validation
#'   functions; thus a prompt name of "clean_data" will assume there is a
#'   function called {api}_clean_data_response_validation and a function called
#'   {api}_clean_data_content_extraction that perform the validation of your
#'   response and extraction of the content.
#' @param rate_limit A numeric value specifying the minimum time (in seconds)
#'   between API calls. Default is 2.
#' @param max_retries An integer specifying the maximum number of retries for
#'   failed API calls. Default is 10.
#' @param temperature A numeric value between 0 and 1 controlling the randomness
#'   of the model's output. Lower values make the output more deterministic.
#'   Default is 0.2.
#' @param max_tokens An integer specifying the maximum number of tokens in the
#'   model's response. Default is 300.
#' @param json_mode A logical value indicating whether the response should be in
#'   JSON format. Default is TRUE.
#' @param system An optional system message (for Claude only).
#'
#' @return A tibble containing the API responses and usage information,
#'   including:
#'   - prompt_id: Identifier for each prompt
#'   - api: The API used
#'   - model: The model used
#'   - input_tokens: Number of tokens in the input
#'   - output_tokens: Number of tokens in the output
#'   - total_tokens: Total number of tokens used
#'   - input_cost: Cost for input tokens
#'   - output_cost: Cost for output tokens
#'   - total_cost: Total cost of the API call
#'   - response: The model's response
#'
#' @details This function is implemented as a generic with methods for different
#'   APIs:
#' - call_api.groq
#' - call_api.claude
#' - call_api.openai
#' - call_api.gemini
#'
#'   Each method handles API-specific details such as endpoint URLs,
#'   authentication, and response parsing.
#'
#'   The function includes built-in error handling and retries for failed API
#'   calls. It also implements rate limiting to prevent exceeding API usage
#'   limits.
#'
#'   For JSON responses, the function includes validation and parsing of the
#'   JSON content.
#'
#' @examples
#' \dontrun{
#' # Assuming 'prompts' is a list of prompts with class 'openai'
#' responses <- call_api(prompts,
#'                       api = "openai",
#'                       model = "gpt-3.5-turbo",
#'                       temperature = 0.7,
#'                       max_tokens = 500)
#' }
#'
#' @seealso [build_prompts_from_files()] for creating prompts to use with this
#'   function.
#'
#' @export
call_api <- function(prompts,
                     model,
                     prompt_name,
                     rate_limit,
                     max_retries,
                     temperature,
                     max_tokens,
                     json_mode,
                     system) {


  UseMethod("call_api", prompts)

}

#' @export
call_api.groq <- function(prompts,
                          model,
                          prompt_name,
                          rate_limit = 2,
                          max_retries = 10,
                          temperature = 0.2,
                          max_tokens = 300,
                          json_mode = TRUE,
                          system = NULL) {

  if(missing(model)) {
    model <- get_default_model(class(prompts)[1])
  }

  if(missing(prompt_name)) {
    if(json_mode) {
      prompt_name <- "json"
    } else {
      prompt_name <- "default"
    }
  }

  validate_args_call_api(prompts,
                         model,
                         prompt_name,
                         rate_limit,
                         max_retries,
                         temperature,
                         max_tokens,
                         json_mode,
                         system)

  response_validation_fun <- paste("groq", prompt_name, "response_validation", sep = "_")
  content_extraction_fun <- paste("groq", prompt_name, "content_extraction", sep = "_")

  prompts |>
    purrr::map(\(x) groq_single_request(x,
                                        model = model,
                                        rate_limit = rate_limit,
                                        max_retries = max_retries,
                                        temperature = temperature,
                                        max_tokens = max_tokens,
                                        json_mode = json_mode,
                                        response_validation_fun = response_validation_fun,
                                        content_extraction_fun = content_extraction_fun)) |>
    purrr::list_rbind(names_to = "prompt_id")

}

groq_single_request <- function(prompt,
                                model,
                                rate_limit,
                                max_retries,
                                temperature,
                                max_tokens,
                                json_mode,
                                response_validation_fun,
                                content_extraction_fun) {

  if(json_mode) {
    response_format = jsonlite::toJSON(list(type = "json_object"),
                                       auto_unbox = TRUE)
  } else {
    response_format = NULL
  }

  body <- jsonlite::toJSON(
    list(
      messages = prompt,
      model = model,
      max_tokens = max_tokens,
      temperature = temperature
    ),
    auto_unbox = TRUE,
    pretty = TRUE
  )

  for(i in 1:max_retries) {
    response <- httr::POST(
      url = "https://api.groq.com/openai/v1/chat/completions",
      httr::add_headers("Authorization" = paste("Bearer", Sys.getenv("GROQ_API_KEY")),
                        "content-type" = "application/json",
                        "response_format" = response_format
      ),
      body = body,
      encode = "json"
    )
    if(httr::http_error(response)) {
      Sys.sleep(i*10)
    } else if(!do.call(response_validation_fun, list(response))) {
      Sys.sleep(i*10)
    } else {
      break
    }
  }

  httr::stop_for_status(response)
  res <- httr::content(response)
  hdr <- httr::headers(response)

  usage <- groq_usage(res)
  content <- do.call(content_extraction_fun, list(res))
  usage$response <- content

  reset_tokens <- hdr$`x-ratelimit-reset-tokens`
  if(stringr::str_detect(reset_tokens, "ms")) {
    reset_tokens <- reset_tokens |>
      readr::parse_number()
    reset_tokens <- reset_tokens/1000
  } else {
    reset_tokens <- reset_tokens |>
      readr::parse_number()
  }

  remaining_tokens <- hdr$`x-ratelimit-remaining-tokens` |>
    readr::parse_number()

  if(reset_tokens > 20 || remaining_tokens < 2000) {
    Sys.sleep(reset_tokens)
  }

  Sys.sleep(rate_limit)

  usage

}

groq_default_response_validation <- function(response) {
  return(TRUE)
}

groq_default_content_extraction <- function(response_content) {
  response_content$choices[[1]]$message$content
}

groq_json_response_validation <- function(response) {
  httr::content(response) |>
    groq_default_content_extraction() |>
    default_json_content_cleaning() |>
    jsonlite::validate()
}

groq_json_content_extraction <- function(response_content) {
  response_content |>
    groq_default_content_extraction() |>
    default_json_content_extraction()
}

groq_usage <- function(response) {
  response$usage |>
    dplyr::as_tibble() |>
    dplyr::mutate(model = response$model) |>
    dplyr::left_join(models_df, by = "model") |>
    dplyr::rename(input_tokens = .data$prompt_tokens,
                  output_tokens = .data$completion_tokens) |>
    dplyr::mutate(input_cost = .data$input_cost * .data$input_tokens/1e6,
                  output_cost = .data$output_cost * .data$output_tokens/1e6,
                  total_cost = .data$input_cost + .data$output_cost) |>
    dplyr::relocate(.data$api, .data$model)
}

#' @export
call_api.claude <- function(prompts,
                            model,
                            prompt_name,
                            rate_limit = 2,
                            max_retries = 10,
                            temperature = 0.2,
                            max_tokens = 300,
                            json_mode = TRUE,
                            system = NULL) {
  if(missing(model)) {
    model <- get_default_model(class(prompts)[1])
  }

  if(missing(prompt_name)) {
    if(json_mode) {
      prompt_name <- "json"
    } else {
      prompt_name <- "default"
    }
  }

  validate_args_call_api(prompts,
                         model,
                         prompt_name,
                         rate_limit,
                         max_retries,
                         temperature,
                         max_tokens,
                         json_mode,
                         system)

  response_validation_fun <- paste("claude", prompt_name, "response_validation", sep = "_")
  content_extraction_fun <- paste("claude", prompt_name, "content_extraction", sep = "_")

  prompts |>
    purrr::map(\(x) claude_single_request(x,
                                          model = model,
                                          rate_limit = rate_limit,
                                          max_retries = max_retries,
                                          temperature = temperature,
                                          max_tokens = max_tokens,
                                          system = system,
                                          response_validation_fun = response_validation_fun,
                                          content_extraction_fun = content_extraction_fun)) |>
    purrr::list_rbind(names_to = "prompt_id")

}

claude_single_request <- function(prompt,
                                  model,
                                  rate_limit,
                                  max_retries,
                                  temperature,
                                  max_tokens,
                                  system,
                                  response_validation_fun,
                                  content_extraction_fun) {

  body <- jsonlite::toJSON(
    list(
      messages = prompt,
      model = model,
      max_tokens = max_tokens,
      temperature = temperature
    ),
    auto_unbox = TRUE,
    pretty = TRUE
  )

  for(i in 1:max_retries) {
    response <- httr::POST(
      url = "https://api.anthropic.com/v1/messages",
      httr::add_headers("x-api-key" = Sys.getenv("ANTHROPIC_API_KEY"),
                        "anthropic-version" = "2023-06-01",
                        "system" = system,
                        "content-type" = "application/json"),
      body = body,
      encode = "json",
      httr::timeout(30)
    )
    if(httr::http_error(response)) {
      msg <- stringr::str_glue("Claude had an error - trying again in {i*10} secs: {httr::content(response)}.")
      cli::cli_warn(msg)
      Sys.sleep(i*10)
    } else if(!do.call(response_validation_fun, list(response))) {
      msg <- stringr::str_glue("Validation error - trying again in {i*10} secs: {claude_default_content_extraction(httr::content(response))}.")
      cli::cli_warn(msg)
      Sys.sleep(i*10)
    } else {
      break
    }
  }

  httr::stop_for_status(response)
  res <- httr::content(response)

  usage <- claude_usage(res)
  content <- do.call(content_extraction_fun, list(res))
  usage$response <- content

  Sys.sleep(rate_limit)

  usage

}

claude_default_response_validation <- function(response) {
  return(TRUE)
}

claude_json_response_validation <- function(response) {
  httr::content(response) |>
    claude_default_content_extraction() |>
    default_json_content_cleaning() |>
    jsonlite::validate()
}

claude_default_content_extraction <- function(response_content) {
  response_content$content[[1]]$text
}

claude_json_content_extraction <- function(response_content) {
  response_content |>
    claude_default_content_extraction() |>
    default_json_content_extraction()
}

claude_usage <- function(response) {
  response$usage |>
    dplyr::as_tibble() |>
    dplyr::mutate(model = response$model) |>
    dplyr::left_join(models_df, by = "model") |>
    dplyr::mutate(input_cost = .data$input_cost * .data$input_tokens/1e6,
                  output_cost = .data$output_cost * .data$output_tokens/1e6,
                  total_cost = .data$input_cost + .data$output_cost) |>
    dplyr::relocate(.data$api, .data$model)
}

#' @export
call_api.openai <- function(prompts,
                            model,
                            prompt_name,
                            rate_limit = 0,
                            max_retries = 10,
                            temperature = 0.2,
                            max_tokens = 300,
                            json_mode = TRUE,
                            system = NULL) {

  if(missing(model)) {
    model <- get_default_model(class(prompts)[1])
  }

  if(missing(prompt_name)) {
    if(json_mode) {
      prompt_name <- "json"
    } else {
      prompt_name <- "default"
    }
  }

  validate_args_call_api(prompts,
                         model,
                         prompt_name,
                         rate_limit,
                         max_retries,
                         temperature,
                         max_tokens,
                         json_mode,
                         system)

  response_validation_fun <- paste("openai", prompt_name, "response_validation", sep = "_")
  content_extraction_fun <- paste("openai", prompt_name, "content_extraction", sep = "_")

  prompts |>
    purrr::map(\(x) openai_single_request(x,
                                          model = model,
                                          rate_limit = rate_limit,
                                          temperature = temperature,
                                          max_tokens = max_tokens,
                                          response_validation_fun = response_validation_fun,
                                          content_extraction_fun = content_extraction_fun)) |>
    purrr::list_rbind(names_to = "prompt_id")

}

openai_single_request <- function(prompt,
                                  model,
                                  rate_limit,
                                  temperature,
                                  max_tokens,
                                  response_validation_fun,
                                  content_extraction_fun) {

  res <- openai::create_chat_completion(model = model,
                                        messages = prompt,
                                        temperature = temperature,
                                        max_tokens = max_tokens)

  usage <- openai_usage(res)
  content <- do.call(content_extraction_fun, list(res))
  usage$response <- content

  Sys.sleep(rate_limit)

  usage

}

openai_default_response_validation <- function(response) {
  return(TRUE)
}

openai_json_response_validation <- function(response) {
  response |>
    openai_default_content_extraction() |>
    default_json_content_cleaning() |>
    jsonlite::validate()
}

openai_default_content_extraction <- function(response_content) {
  response_content$choices$message.content
}

openai_json_content_extraction <- function(response_content) {
  response_content |>
    openai_default_content_extraction() |>
    default_json_content_extraction()
}

openai_usage <- function(response) {
  response$usage |>
    dplyr::as_tibble() |>
    dplyr::mutate(model = response$model) |>
    dplyr::left_join(models_df, by = "model") |>
    dplyr::rename(input_tokens = .data$prompt_tokens,
                  output_tokens = .data$completion_tokens) |>
    dplyr::mutate(input_cost = .data$input_cost * .data$input_tokens/1e6,
                  output_cost = .data$output_cost * .data$output_tokens/1e6,
                  total_cost = .data$input_cost + .data$output_cost) |>
    dplyr::relocate(.data$api, .data$model)
}

#' @export
call_api.gemini <- function(prompts,
                            model,
                            prompt_name,
                            rate_limit = 2,
                            max_retries = 10,
                            temperature = 0.2,
                            max_tokens = 300,
                            json_mode = TRUE,
                            system = NULL) {

  if(missing(model)) {
    model <- get_default_model(class(prompts)[1])
  }

  if(missing(prompt_name)) {
    if(json_mode) {
      prompt_name <- "json"
    } else {
      prompt_name <- "default"
    }
  }

  validate_args_call_api(prompts,
                         model,
                         prompt_name,
                         rate_limit,
                         max_retries,
                         temperature,
                         max_tokens,
                         json_mode,
                         system)

  response_validation_fun <- paste("gemini", prompt_name, "response_validation", sep = "_")
  content_extraction_fun <- paste("gemini", prompt_name, "content_extraction", sep = "_")

  prompts |>
    purrr::map(\(x) gemini_single_request(x,
                                          model = model,
                                          rate_limit = rate_limit,
                                          max_retries = max_retries,
                                          temperature = temperature,
                                          max_tokens = max_tokens,
                                          response_validation_fun = response_validation_fun,
                                          content_extraction_fun = content_extraction_fun)) |>
    purrr::list_rbind(names_to = "prompt_id")

}

gemini_single_request <- function(prompt,
                                  model,
                                  rate_limit,
                                  max_retries,
                                  temperature,
                                  max_tokens,
                                  response_validation_fun,
                                  content_extraction_fun) {

  model_query <- paste0(model, ":generateContent")

  body <- jsonlite::toJSON(
    list(
      contents = prompt,
      generationConfig = list(
        temperature = temperature,
        maxOutputTokens = max_tokens
      )
    ),
    auto_unbox = TRUE,
    pretty = TRUE
  )


 response <- httr::POST(
      url = paste0("https://generativelanguage.googleapis.com/v1beta/models/", model_query),
      query = list(key = Sys.getenv("GEMINI_API_KEY")),
      httr::content_type_json(),
      encode = "json",
      body = body
    )

 httr::stop_for_status(response)
 res <- httr::content(response)
 usage <- gemini_usage(res, model)
 content <- do.call(content_extraction_fun, list(res))
 usage$response <- content

 Sys.sleep(rate_limit)

 usage
}

gemini_default_response_validation <- function(response) {
  return(TRUE)
}

gemini_json_response_validation <- function(response) {
  response |>
    gemini_default_content_extraction() |>
    default_json_content_cleaning() |>
    jsonlite::validate()
}

gemini_default_content_extraction <- function(response_content) {
  response_content$candidates[[1]]$content$parts[[1]]$text
}

gemini_json_content_extraction <- function(response_content) {
  response_content |>
    gemini_default_content_extraction() |>
    default_json_content_extraction()
}

gemini_usage <- function(response, model) {
  if(is.null(response$usageMetadata$candidatesTokenCount)) {
    response$usageMetadata$candidatesTokenCount <- 0
  }
  response$usageMetadata |>
    dplyr::as_tibble() |>
    dplyr::mutate(model = model) |>
    dplyr::left_join(models_df, by = "model") |>
    dplyr::rename(input_tokens = .data$promptTokenCount,
                  output_tokens = .data$candidatesTokenCount,
                  total_tokens = .data$totalTokenCount) |>
    dplyr::mutate(input_cost = .data$input_cost * .data$input_tokens/1e6,
                  output_cost = .data$output_cost * .data$output_tokens/1e6,
                  total_cost = .data$input_cost + .data$output_cost) |>
    dplyr::relocate(.data$api, .data$model)
}
default_json_content_extraction <- function(json_string) {
  json_string |>
    default_json_content_cleaning() |>
    jsonlite::fromJSON() |>
    dplyr::as_tibble()
}

default_json_content_cleaning <- function(json_string) {
  json_string |>
    stringr::str_remove("```$") |>
    stringr::str_remove("```json( )?")  |>
    stringr::str_replace(stringr::regex("\\}\n.+", dotall = TRUE), "}")
}


