% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llamafile.R
\name{llamafile_single_request}
\alias{llamafile_single_request}
\title{Execute a Single Request to a Llamafile Local API}
\usage{
llamafile_single_request(
  prompt,
  model = "LLaMA_CPP",
  n_candidates = 1,
  max_retries = 10,
  temperature = 0.2,
  max_tokens = 300,
  json_mode = FALSE,
  system,
  response_validation_fun,
  content_extraction_fun,
  pause_cap = 1200,
  quiet = FALSE
)
}
\arguments{
\item{prompt}{A list of messages, each with a role (e.g., "user" or
"assistant") and content, used as input for the model. Must be in the
format \code{list(list(role = "user", content = "Hello world!"))}.}

\item{model}{The model to use for generating responses, e.g.,
"LLaMA_CPP".}

\item{n_candidates}{The number of response candidates to generate. Defaults
to 1.}

\item{max_retries}{The maximum number of retry attempts in case of request
failures. Defaults to 10.}

\item{temperature}{A numeric value between 0 and 1 that controls the
randomness of the response. Higher values make the output more random.
Defaults to 0.2.}

\item{max_tokens}{The maximum number of tokens to include in the response.
Defaults to 300.}

\item{json_mode}{A logical value indicating whether the response should be
parsed as JSON. Defaults to \code{FALSE}.}

\item{system}{Optional system message providing instructions or context for
the model.}

\item{response_validation_fun}{A function to validate the response received
from the API. Defaults to \code{llamafile_default_response_validation()} if not
provided.}

\item{content_extraction_fun}{A function to extract the desired content from
the API response. If not provided, a default extraction function is used
depending on the value of \code{json_mode}.}

\item{pause_cap}{A numeric value representing the maximum pause duration (in
seconds) between retries. Defaults to 1200.}

\item{quiet}{A logical value indicating whether the function should suppress
messages during retries. Defaults to \code{FALSE}.}
}
\value{
A tibble containing the usage statistics (tokens used) and the
generated response(s).
}
\description{
This function sends a single prompt to a locally running Llamafile instance
using the OpenAI-compatible API endpoint. It allows users to customize parameters
such as model, number of candidates, and more. It also handles retries and extracts
the relevant response content.
}
