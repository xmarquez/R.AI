% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mistral.R
\name{call_api.mistral}
\alias{call_api.mistral}
\title{Call the Mistral API}
\usage{
\method{call_api}{mistral}(
  prompts,
  model,
  prompt_name,
  n_candidates = 1,
  max_retries = 10,
  temperature = 0.2,
  top_p = 1,
  max_tokens = 300,
  stop = NULL,
  random_seed = NULL,
  presence_penalty = 0,
  frequency_penalty = 0,
  safe_prompt = FALSE,
  pause_cap = 1200,
  log = TRUE
)
}
\arguments{
\item{prompts}{A list of messages, each with a role (e.g., "user" or
"assistant") and content, used as input for the model. Must be in the
format \code{list(list(role = "user", content = "Hello world!"))}.}

\item{model}{The model to use for generating responses. Defaults to a
suitable model depending on the type of prompts. If not provided, it will
use a default model determined by the prompt type.}

\item{prompt_name}{A character string specifying the prompt type. If not
provided, it defaults to "json" if \code{json_mode} is true, otherwise "default".}

\item{n_candidates}{The number of response candidates to generate. Defaults
to 1.}

\item{max_retries}{The maximum number of retry attempts in case of request
failures. Defaults to 10.}

\item{temperature}{A numeric value between 0 and 1 that controls the
randomness of the response. Higher values make the output more random.
Defaults to 0.2.}

\item{top_p}{A numeric value that specifies nucleus sampling probability.
It controls the probability mass of the tokens considered at each step.
Defaults to 1.}

\item{max_tokens}{The maximum number of tokens to include in the response.
Defaults to 300.}

\item{stop}{Optional. A character vector specifying the sequences where the
model should stop generating further tokens. Defaults to \code{NULL}.}

\item{random_seed}{An optional integer specifying a random seed for
reproducibility. Defaults to \code{NULL}.}

\item{presence_penalty}{A numeric value that encourages the model to talk about
new topics. Defaults to 0.}

\item{frequency_penalty}{A numeric value that decreases the likelihood of repeating
the same line verbatim. Defaults to 0.}

\item{safe_prompt}{A logical value indicating whether to inject a safety prompt
before all conversations. Defaults to \code{FALSE}. When set to \code{TRUE}, an additional
safety layer is added to the prompt to ensure content safety.}

\item{pause_cap}{A numeric value representing the maximum pause duration (in
seconds) between retries. Defaults to 1200.}

\item{log}{A logical value indicating whether the function should log the API
request details. Defaults to \code{TRUE}.}

\item{stream}{Optional. A logical value that, if set to \code{TRUE}, indicates that the
model's output should be streamed back in chunks. Defaults to \code{FALSE}.}
}
\value{
A tibble containing the usage statistics (tokens used) and the
generated response(s).
}
\description{
This function sends a single prompt to the Mistral API for processing,
allowing users to customize parameters such as the model, number of candidates,
and more. It also handles retries and extracts the relevant response content.
}
