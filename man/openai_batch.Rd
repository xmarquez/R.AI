% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/openai-batch.R
\name{openai_batch}
\alias{openai_batch}
\title{Execute OpenAI Batch Workflow}
\usage{
openai_batch(prompts, model, timeout = 3600, max_tokens = 300, quiet = FALSE)
}
\arguments{
\item{prompts}{A list of prompts, each containing messages to send to the
model, generated by \code{\link[=build_prompts_from_files]{build_prompts_from_files()}}.}

\item{model}{The model to use for processing the prompts, e.g.,
"gpt-3.5-turbo". If not specified, the function will use a default model.}

\item{timeout}{A numeric value representing the time (in seconds) to wait
between polling attempts. Defaults to 3600 seconds (1 hour).}

\item{max_tokens}{The maximum number of tokens to generate for each response.
Defaults to 300.}

\item{quiet}{A logical value indicating whether the function should suppress
messages during the process. Defaults to \code{FALSE}.}
}
\value{
A tibble or a character vector containing each line of the .jsonl
result file, depending on the value of \code{tidy} in
\code{openai_poll_and_retrieve_results}.
}
\description{
This function orchestrates the entire OpenAI batch process: it formats the
prompts, uploads the batch file, creates the batch request, and polls until
the results are ready.
}
