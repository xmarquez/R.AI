% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/prompt-formatter.R
\name{format_chat}
\alias{format_chat}
\alias{format_chat.default}
\alias{format_chat.claude}
\alias{format_chat.gemini}
\title{Format Chat Prompts for Different Backends}
\usage{
format_chat(api, user, system, data, rtika, quiet, ...)

\method{format_chat}{default}(api, user, data, system, rtika = FALSE, quiet = TRUE, ...)

\method{format_chat}{claude}(api, user, data, system, cache, rtika = FALSE, quiet = TRUE, ...)

\method{format_chat}{gemini}(
  api,
  user,
  data,
  system,
  cache = NULL,
  cache_name = NULL,
  rtika = FALSE,
  quiet = TRUE,
  ...
)
}
\arguments{
\item{api}{A character string or an object indicating which backend is used,
e.g. \code{"openai"}, \code{"claude"}, \code{"gemini"}, etc. This is typically set
automatically by the calling functions or by a \code{class()} on the object.}

\item{user}{A character vector of user inputs. May be file paths, URLs, or
direct text. Internally, a function attempts to convert
images and PDFs into base64 or text, depending on the backend capabilities.}

\item{system}{(Optional) A string containing the "system" instructions or
context. Some backends (e.g. OpenAI, Claude) treat system prompts as a
separate role. Others (e.g. Gemini) do so differently.}

\item{data}{(Optional) A data frame containing fields to be templated into
user prompts. If non-empty, each row is used to glue data into \code{user}
content with \code{\link[stringr:str_glue]{stringr::str_glue_data()}}.}

\item{rtika}{Logical. If \code{TRUE}, and a file is a PDF, attempts to parse text
from PDF using \code{rtika::tika_text()}. Defaults to \code{FALSE}.}

\item{quiet}{Logical. If \code{FALSE}, shows messages about file reading or PDF
parsing. Defaults to \code{TRUE}.}

\item{...}{Additional arguments (not currently used).}

\item{cache}{For the \strong{Claude} method, a numeric vector specifying which
prompts should get \code{cache_control = "ephemeral"}. For the \strong{Gemini}
method, a numeric vector of row indices to extract as cached content. If
\code{NULL}, no caching is applied.}

\item{cache_name}{For the \strong{Gemini} method, an optional string used as a
display name for the cache. If none is provided, an auto-generated name is
used (e.g. a hash from \code{\link[digest:digest]{digest::digest()}}).}
}
\value{
A list (or nested list) structured to match the target API’s
expectations. The resulting object typically has a class of the form
\code{c("{api}_list", "list")} or \code{c(api, "combined_prompts", "list")}. This
object is then passed to the corresponding \verb{chat.*} function for actual API
calls.
}
\description{
\code{format_chat()} is a generic function that takes user/system
inputs, plus optionally some caching parameters or data, and returns a
structured list of messages suitable for a specific backend (OpenAI,
Claude, Gemini, etc.).

This function normalizes inputs (like reading local files, inlining images,
or embedding PDFs) and produces the final message format expected by each
model's API. The exact structure depends on the S3 method:
\itemize{
\item \strong{\code{format_chat.default()}}: A baseline fallback, typically for OpenAI-like
prompt structures.
\item \strong{\code{format_chat.claude()}}: Structures text, images, or documents for
Anthropic Claude, including optional ephemeral caching (\code{cache_control}).
\item \strong{\code{format_chat.gemini()}}: Structures text or file references for Google
Gemini, optionally extracting some user messages for caching via \code{cache}.
}
}
\details{
\strong{Default Method}
The default method is a fallback for an “OpenAI-like” structure, creating
simple role/content pairs. Any PDFs in \code{user} become text (file content) and
any images remain as an "image_url" block (this includes image urls or image
files, which are automatically encoded in base64).

\strong{Claude Method}
Accepts numeric \code{cache} indices, marking those rows in the user’s prompt for
ephemeral caching with \code{cache_control = list(type = "ephemeral")}. Produces a
list of messages in Anthropic’s \code{messages} format, each containing a \code{role}
plus \code{content} with optional ephemeral blocks.

\strong{Gemini Method}
Splits user prompts into “final” vs. “cache” blocks if their row index is
listed in \code{cache}. The cached blocks are attached as an attribute named
\code{"cache"} on the returned list, so that subsequent calls to
\code{\link[=chat.gemini_list]{chat.gemini_list()}} can create or retrieve the cache. For inline data, PDFs,
or local images, these are encoded into \code{inlineData} or \code{fileData}.
}
\examples{
\dontrun{
# Basic usage with default method:
msgs <- format_chat("openai", user = "Hello world")

# With Claude ephemeral caching:
claude_msgs <- format_chat("claude",
  user = c("First user prompt", "Second user prompt"),
  cache = 2
)

# With Gemini caching
gemini_msgs <- format_chat("gemini",
  user = c("Prompt 1", "Prompt 2"),
  cache = 1
)
}

}
\seealso{
\code{\link[=chat]{chat()}} for sending the resulting structured prompts to the
selected backend, and the content documentation for
\href{https://platform.openai.com/docs/api-reference/chat/create}{OpenAI},
\href{https://docs.anthropic.com/claude/api-reference}{Anthropic}, and \href{https://ai.google.dev/api/}{Google Gemini}.
}
